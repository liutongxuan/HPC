build with omp:
g++ matmul.cc -std=c++11 -O2 -mavx2 -msse4.1 -msse4.2 -mfma -mfpmath=both -fopenmp -g -march=native -o matmul

Summary:
Case m,n,k = 1024,2048,512 matmul，loop unrolling_1x4 improve 2x faster，sse compare with loop unrolling_1x4 improve 2x faster，avx256 compare with sse improve 2x faster，totally 8x faster than initial version.

gcc5 is not support avx512 option which need higher than gcc6.0. And also need CPU skylake. I guess if avx512 would get 2x faster.

Register index & aligned_sse improve performance as well (aligned_sse compare with unaligned sse get 6% faster，register index compare with unregister index would improve 20%)

pack & 4x4 loop make performance worse。

Reference:
https://jackwish.net/gemm-optimization.html
https://github.com/xianyi/OpenBLAS
http://apfel.mathematik.uni-ulm.de/~lehn/sghpc/
https://github.com/flame/how-to-optimize-gemm/wiki
https://software.intel.com/sites/landingpage/IntrinsicsGuide/
https://www.cs.uaf.edu/2009/fall/cs301/lecture/11_13_sse_intrinsics.html)
